{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Challenge\n",
    "Do a little scraping or API-calling of your own. Pick a new website and see what you can get out of it. Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.\n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)\n",
    "2) Iterate over multiple pages/queries\n",
    "3) Save the data to your computer\n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest. Write up a report from scraping code to summary and share it with your mentor.\n",
    "\n",
    "## Aim\n",
    "\n",
    "Scrape all current abstracts from Nature Microbiology, inclucing Authors, affiliatiotions and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "#https://stackoverflow.com/questions/23585992/making-post-request-using-scrapy\n",
    "#https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get me the pubmed ids of all Nature Microbiology paper\n",
    "class PubmedIDSpider(scrapy.Spider):\n",
    "    name = \"PubmedIDSpider\"\n",
    "    start_urls = [\n",
    "            \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json&retmax=1000&term=Nat+Microbiol%5BJOURNAL%5D\"\n",
    "    ]\n",
    "    # \n",
    "    #retmode=json, returns json file instead of xml\n",
    "    # retmax=1000, returns 1000 results instead of 20\n",
    "    # if there are more than 1000 results switch to the next page by using &retstart=1000 into the url\n",
    "    # eutils/esummary.fcgi? gives abstract information without abstract. Example http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&rettype=abstract&id=25081398,24792655\n",
    "    # eutils/efetch.fcgi? gives the abstract. Example: http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=text&rettype=abstract&id=25081398\n",
    "    \n",
    "    def parse(self, response): \n",
    "        json_data = json.loads(response.body)\n",
    "        results=json_data[\"esearchresult\"][\"idlist\"]\n",
    "        \n",
    "        print('Number of IDs',len(results))\n",
    "        yield {'IDs':results}\n",
    "            \n",
    "             \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'csv',\n",
    "    'FEED_URI': 'PubmedIDs.csv',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'NadineRuecker(nadineruecker@gmail.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Starting the crawler with our spider.\n",
    "process.crawl(PubmedIDSpider)\n",
    "process.start()\n",
    "print('Sucess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "IDs= pd.read_csv('PubmedIDs.csv').iloc[0,0].split(\",\")  # comes in a weird format, so the list of IDs has to be extracted from the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&rettype=abstract&id=30787480', 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&rettype=abstract&id=30787479', 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&rettype=abstract&id=30787478']\n"
     ]
    }
   ],
   "source": [
    "start_url='http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&rettype=abstract&id='\n",
    "start_urls = [start_url+ID for ID in IDs]\n",
    "print(start_urls[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now use those IDs to call another Spider and read the abstract information\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "\n",
    "# get me the pubmed ids of all Nature Microbiology paper\n",
    "class PubmedAbstractSpider(scrapy.Spider):\n",
    "    name = \"PubmedAbstractSpider\"\n",
    "    start_url='http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&rettype=abstract&id='\n",
    "    start_urls = [start_url+ID for ID in IDs]\n",
    "    # \n",
    "    # retmode=json, returns json file instead of xml\n",
    "    # retmax=1000, returns 1000 results instead of 20\n",
    "    # if there are more than 1000 results switch to the next page by using &retstart=1000 into the url\n",
    "    # eutils/esummary.fcgi? gives abstract information without abstract. Example http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=pubmed&retmode=json&rettype=abstract&id=25081398,24792655\n",
    "    # eutils/efetch.fcgi? gives the abstract. Example: http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=text&rettype=abstract&id=25081398\n",
    "\n",
    "    def parse(self, response): \n",
    "        json_data = json.loads(response.body)\n",
    "\n",
    "        print('Number of IDs',len(results))\n",
    "        yield {'IDs':ID,\n",
    "               'Pubdate':json_data['result'][ID]['pubdate'],\n",
    "               'Authors':json_data['result'][ID]['authors'],\n",
    "               'LastAuthor':json_data['result'][ID]['lastauthor'],\n",
    "               'Title':json_data['result'][ID]['Title']\n",
    "              }\n",
    "            \n",
    "             \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'ArticleData.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'NadineRuecker(nadineruecker@gmail.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucess\n"
     ]
    }
   ],
   "source": [
    "# Starting the crawler with our spider.\n",
    "process.crawl(PubmedAbstractSpider)\n",
    "process.start()\n",
    "print('Sucess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PubmedSpider(Spider):\n",
    "    name = \"pubmed\"\n",
    "    cur_page = 1\n",
    "    max_page = 3\n",
    "    start_urls = [\n",
    "            \"https://www.ncbi.nlm.nih.gov/pubmed/?term=Nat+Microbiol%5BJOURNAL%5D\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        sel = Selector(response)\n",
    "        pubmed_results = sel.xpath('//div[@class=\"rslt\"]')\n",
    "        #next_page_url = sel.xpath('//div[@id=\"gs_n\"]//td[@align=\"left\"]/a/@    href').extract()[0]\n",
    "        self.cur_page = self.cur_page + 1\n",
    "        print 'cur_page ','*' * 30, self.cur_page\n",
    "\n",
    "        form_data = {'term':'Nat+Microbiol%5BJOURNAL%5D',\n",
    "                    'EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Entrez_Pager.Page':'results',\n",
    "                    'email_subj':'cancer+drug+toxic+-+PubMed',\n",
    "                    'EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Entrez_Pager.CurrPage':str(self.cur_page),\n",
    "                    'email_subj2':'cancer+drug+toxic+-+PubMed',\n",
    "                    'EntrezSystem2.PEntrez.DbConnector.LastQueryKey':'2',\n",
    "                    'EntrezSystem2.PEntrez.DbConnector.Cmd':'PageChanged',\n",
    "                    'p%24a':'EntrezSystem2.PEntrez.PubMed.Pubmed_ResultsPanel.Entrez_Pager.Page',\n",
    "                    'p%24l':'EntrezSystem2',\n",
    "                    'p%24':'pubmed',\n",
    "                    }\n",
    "\n",
    "        for pubmed_result in pubmed_results:\n",
    "            item = PubmedItem()\n",
    "\n",
    "            item['title'] = lxml.html.fromstring(pubmed_result.xpath('.//a')[0].extract()).text_content()\n",
    "            item['link'] = pubmed_result.xpath('.//p[@class=\"title\"]/a/@href').extract()[0]\n",
    "\n",
    "            #modify following lines\n",
    "            if self.cur_page < self.max_page:\n",
    "                yield FormRequest(\"https://www.ncbi.nlm.nih.gov/pubmed/?term=Nat+Microbiol%5BJOURNAL%5D\",formdata = form_data,\n",
    "                callback = self.parse2, method=\"POST\")\n",
    "\n",
    "            yield item\n",
    "\n",
    "    def parse2(self, response):\n",
    "        with open('response_html', 'w') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class NCBISpider(scrapy.Spider):\n",
    "    name = \"NCBI-Spi\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term=cancer+toxic+drug'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
